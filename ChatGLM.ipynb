{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 删除可能存在的Domi仓库\n",
        "!rm -rf Domi\n",
        "# 清空content\n",
        "!rm -rf *\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "cMDMdeQKPFyF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe60c4a3-8e15-4d19-e200-d03dc5c2cf5a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Apr 25 06:07:56 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    43W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EqmZjGhCAuC_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4152e11-1c13-4e32-c0fc-cfe663f1131e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Domi'...\n",
            "remote: Enumerating objects: 68, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 68 (delta 5), reused 2 (delta 1), pack-reused 54\u001b[K\n",
            "Unpacking objects: 100% (68/68), 974.63 KiB | 7.38 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "# 下载仓库\n",
        "!git clone https://github.com/pupilshuo/Domi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cp -r /content/Domi/* /content/"
      ],
      "metadata": {
        "id": "2HXGl-reA8ao"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 配置环境\n",
        "!pip install -U deep_training cpm_kernels icetk transformers deepspeed"
      ],
      "metadata": {
        "id": "gScR5qXqBATR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50e568a6-9059-4920-ffd0-0529bc92aa40"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deep_training\n",
            "  Downloading deep_training-0.1.2.post2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.7/244.7 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cpm_kernels\n",
            "  Downloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.6/416.6 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting icetk\n",
            "  Downloading icetk-0.0.7-py3-none-any.whl (16 kB)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m109.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deepspeed\n",
            "  Downloading deepspeed-0.9.1.tar.gz (766 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.2/766.2 kB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from deep_training) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from deep_training) (1.22.4)\n",
            "Collecting fastdatasets<=1,>=0.9.6\n",
            "  Downloading fastdatasets-0.9.6-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from deep_training) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from deep_training) (1.10.1)\n",
            "Collecting tfrecords<=1,>=0.2.4\n",
            "  Downloading tfrecords-0.2.5-cp39-cp39-manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting seqmetric\n",
            "  Downloading seqmetric-0.1.2-py3-none-any.whl (17 kB)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.98-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning>=2\n",
            "  Downloading pytorch_lightning-2.0.2-py3-none-any.whl (719 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.0/719.0 kB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from deep_training) (4.65.0)\n",
            "Collecting protobuf<3.19\n",
            "  Downloading protobuf-3.18.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from icetk) (3.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from icetk) (2.27.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from icetk) (0.15.1+cu118)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.0-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.2/224.2 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hjson\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from deepspeed) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.9/dist-packages (from deepspeed) (9.0.0)\n",
            "Requirement already satisfied: pydantic<2.0.0 in /usr/local/lib/python3.9/dist-packages (from deepspeed) (1.10.7)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from deepspeed) (2.0.0+cu118)\n",
            "Collecting data-serialize>=0.2.1\n",
            "  Downloading data_serialize-0.2.1-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Collecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lightning-utilities>=0.7.0\n",
            "  Downloading lightning_utilities-0.8.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->deepspeed) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch->deepspeed) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->deepspeed) (3.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->deepspeed) (1.11.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->deepspeed) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->deepspeed) (16.0.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->icetk) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->icetk) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->icetk) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->icetk) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->deep_training) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->deep_training) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->icetk) (8.4.0)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch->deepspeed) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->deepspeed) (1.3.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->huggingface-hub<1.0,>=0.11.0->transformers) (23.1.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.3/269.3 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.9.1-py3-none-any.whl size=798586 sha256=b157cf1f89b44d1b0653584e272b039a9730da4f00308701ad42e89c15507c21\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/1a/4d/2cc6f7728fa1aa5ef4cfec49c547155add414e562afe5e6055\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: tokenizers, sentencepiece, ninja, hjson, cpm_kernels, tfrecords, seqmetric, protobuf, multidict, lightning-utilities, frozenlist, async-timeout, yarl, huggingface-hub, data-serialize, aiosignal, transformers, fastdatasets, aiohttp, torchmetrics, pytorch-lightning, icetk, deepspeed, deep_training\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.18.3 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.18.3 which is incompatible.\n",
            "tensorflow-hub 0.13.0 requires protobuf>=3.19.6, but you have protobuf 3.18.3 which is incompatible.\n",
            "tensorboard 2.12.2 requires protobuf>=3.19.6, but you have protobuf 3.18.3 which is incompatible.\n",
            "proto-plus 1.22.2 requires protobuf<5.0.0dev,>=3.19.0, but you have protobuf 3.18.3 which is incompatible.\n",
            "googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.18.3 which is incompatible.\n",
            "google-cloud-translate 3.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.18.3 which is incompatible.\n",
            "google-cloud-language 2.9.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.18.3 which is incompatible.\n",
            "google-cloud-firestore 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.18.3 which is incompatible.\n",
            "google-cloud-datastore 2.15.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.18.3 which is incompatible.\n",
            "google-cloud-bigquery 3.9.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.18.3 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.19.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.18.3 which is incompatible.\n",
            "google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.18.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 cpm_kernels-1.0.11 data-serialize-0.2.1 deep_training-0.1.2.post2 deepspeed-0.9.1 fastdatasets-0.9.6 frozenlist-1.3.3 hjson-3.1.0 huggingface-hub-0.14.0 icetk-0.0.7 lightning-utilities-0.8.0 multidict-6.0.4 ninja-1.11.1 protobuf-3.18.3 pytorch-lightning-2.0.2 sentencepiece-0.1.98 seqmetric-0.1.2 tfrecords-0.2.5 tokenizers-0.13.3 torchmetrics-0.11.4 transformers-4.28.1 yarl-1.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install protobuf==3.20.0"
      ],
      "metadata": {
        "id": "fWNs4wyfbMvw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49eecec7-aa00-46bb-84be-9d7e482f19a5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting protobuf==3.20.0\n",
            "  Downloading protobuf-3.20.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.18.3\n",
            "    Uninstalling protobuf-3.18.3:\n",
            "      Successfully uninstalled protobuf-3.18.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.0 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.20.0 which is incompatible.\n",
            "icetk 0.0.7 requires protobuf<3.19, but you have protobuf 3.20.0 which is incompatible.\n",
            "googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-translate 3.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-language 2.9.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-firestore 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-datastore 2.15.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery 3.9.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.19.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 录入数据\n",
        "# 将要加入的数据上传colab\n",
        "!python merge.py -path finetuning_data"
      ],
      "metadata": {
        "id": "F-89yNkKBEC9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f0ef77c-dbb5-4d50-cc34-c5e7e9025854"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['adivice.json', 'lyric.json', 'glm.json', 'diet.json', 'climate.json', 'orgnazation.json', 'application.json', 'alpaca_data-0-3252.json', 'academy.json', 'threenations.json', 'party.json', 'in.json', 'qa_sample.json', 'computer.json', 'experience.json', 'other.json', 'translate.json', 'noun.json', 'moods.json']\n",
            "finetuning_data/adivice.json\n",
            "finetuning_data/lyric.json\n",
            "finetuning_data/glm.json\n",
            "finetuning_data/diet.json\n",
            "finetuning_data/climate.json\n",
            "finetuning_data/orgnazation.json\n",
            "finetuning_data/application.json\n",
            "finetuning_data/alpaca_data-0-3252.json\n",
            "finetuning_data/academy.json\n",
            "finetuning_data/threenations.json\n",
            "finetuning_data/party.json\n",
            "finetuning_data/in.json\n",
            "finetuning_data/qa_sample.json\n",
            "finetuning_data/computer.json\n",
            "finetuning_data/experience.json\n",
            "finetuning_data/other.json\n",
            "finetuning_data/translate.json\n",
            "finetuning_data/noun.json\n",
            "finetuning_data/moods.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 开启数据预处理，请记得如果数据集有更换请删除临时文件\n",
        "# 注意：data文件夹下若没有output.json，将output.json 加入data文件夹下 \n",
        "!python data_utils.py"
      ],
      "metadata": {
        "id": "_xXfVA7-E4_d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff617e0a-1f36-409d-e680-f4944592ce2b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmpi9ofprv0\n",
            "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmpi9ofprv0/_remote_module_non_scriptable.py\n",
            "2023-04-25 06:22:21.099515: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:numexpr.utils:Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
            "INFO:lightning_fabric.utilities.seed:Global seed set to 42\n",
            "ChatGLMConfig {\n",
            "  \"architectures\": [\n",
            "    \"ChatGLMModel\"\n",
            "  ],\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"configuration_chatglm.ChatGLMConfig\",\n",
            "    \"AutoModel\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
            "    \"AutoModelForSeq2SeqLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\"\n",
            "  },\n",
            "  \"bos_token_id\": 130004,\n",
            "  \"eos_token_id\": 130005,\n",
            "  \"gmask_token_id\": 130001,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"initializer_weight\": false,\n",
            "  \"inner_hidden_size\": 16384,\n",
            "  \"layernorm_epsilon\": 1e-05,\n",
            "  \"mask_token_id\": 130000,\n",
            "  \"max_sequence_length\": 2048,\n",
            "  \"model_type\": \"chatglm\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_layers\": 28,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"position_encoding_2d\": true,\n",
            "  \"pre_seq_len\": null,\n",
            "  \"precision\": 16,\n",
            "  \"prefix_projection\": false,\n",
            "  \"quantization_bit\": 0,\n",
            "  \"return_dict\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"learning_rate_for_task\": 2e-05\n",
            "  },\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.28.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 130528\n",
            "}\n",
            "\n",
            "INFO:root:make_dataset ./data/output.json train...\n",
            "INFO:root:make data ./output/dataset_file_0_dupe_factor_0-train.record...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5U4sDwY7R2vF",
        "outputId": "d987b40a-3eff-4ebb-f9ad-4b8e34b098ae"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmpsqcvhiny\n",
            "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmpsqcvhiny/_remote_module_non_scriptable.py\n",
            "2023-04-25 06:22:45.577189: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:numexpr.utils:Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
            "INFO:lightning_fabric.utilities.seed:Global seed set to 42\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "ChatGLMConfig {\n",
            "  \"architectures\": [\n",
            "    \"ChatGLMModel\"\n",
            "  ],\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"configuration_chatglm.ChatGLMConfig\",\n",
            "    \"AutoModel\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
            "    \"AutoModelForSeq2SeqLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\"\n",
            "  },\n",
            "  \"bos_token_id\": 130004,\n",
            "  \"eos_token_id\": 130005,\n",
            "  \"gmask_token_id\": 130001,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"initializer_weight\": false,\n",
            "  \"inner_hidden_size\": 16384,\n",
            "  \"layernorm_epsilon\": 1e-05,\n",
            "  \"mask_token_id\": 130000,\n",
            "  \"max_sequence_length\": 2048,\n",
            "  \"model_type\": \"chatglm\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_layers\": 28,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"position_encoding_2d\": true,\n",
            "  \"pre_seq_len\": null,\n",
            "  \"precision\": 16,\n",
            "  \"prefix_projection\": false,\n",
            "  \"quantization_bit\": 0,\n",
            "  \"return_dict\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"learning_rate_for_task\": 2e-05\n",
            "  },\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.28.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 130528\n",
            "}\n",
            "\n",
            "INFO:root:make_dataset ./data/output.json train...\n",
            "INFO:root:make data ./output/dataset_file_0_dupe_factor_0-train.record...\n",
            "TrainingArguments(optimizer='lion', scheduler_type='linear', scheduler=None, adv=None, hierarchical_position=None, learning_rate=2e-05, learning_rate_for_task=2e-05, max_epochs=38, max_steps=-1, optimizer_betas=(0.9, 0.999), adam_epsilon=1e-08, gradient_accumulation_steps=1, max_grad_norm=1.0, weight_decay=0, warmup_steps=0, train_batch_size=4, eval_batch_size=2, test_batch_size=2, seed=42)\n",
            "ModelArguments(model_name_or_path='THUDM/chatglm-6b', model_type='chatglm', config_overrides=None, config_name='./config/config.json', tokenizer_name='THUDM/chatglm-6b', cache_dir=None, do_lower_case=False, use_fast_tokenizer=False, model_revision='main', use_auth_token=False)\n",
            "Loading checkpoint shards: 100% 8/8 [00:10<00:00,  1.37s/it]\n",
            "****************************** lora info\n",
            "trainable params: 1835008 || all params: 6175121408 || trainable%: 0.029716144489446126\n",
            "****************************** total 1182\n",
            "INFO:root:load dataset to memory...\n",
            "/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "  rank_zero_warn(\n",
            "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.utilities.rank_zero:Loading `train_dataloader` to estimate number of stepping batches.\n",
            "/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name                                  | Type      | Params\n",
            "--------------------------------------------------------------------\n",
            "0 | _TransformerLightningModule__backbone | LoraModel | 6.2 B \n",
            "--------------------------------------------------------------------\n",
            "1.8 M     Trainable params\n",
            "6.2 B     Non-trainable params\n",
            "6.2 B     Total params\n",
            "24,700.486Total estimated model params size (MB)\n",
            "Epoch 0:  41% 120/295 [00:29<00:43,  4.06it/s, v_num=1, loss=1.960]Traceback (most recent call last):\n",
            "  File \"/content/train.py\", line 178, in <module>\n",
            "    trainer.fit(pl_model, train_dataloaders=train_datasets)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 520, in fit\n",
            "    call._call_and_handle_interrupt(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/call.py\", line 44, in _call_and_handle_interrupt\n",
            "    return trainer_fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 559, in _fit_impl\n",
            "    self._run(model, ckpt_path=ckpt_path)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 935, in _run\n",
            "    results = self._run_stage()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 978, in _run_stage\n",
            "    self.fit_loop.run()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 201, in run\n",
            "    self.advance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 354, in advance\n",
            "    self.epoch_loop.run(self._data_fetcher)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 133, in run\n",
            "    self.advance(data_fetcher)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 218, in advance\n",
            "    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/optimization/automatic.py\", line 185, in run\n",
            "    self._optimizer_step(kwargs.get(\"batch_idx\", 0), closure)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/optimization/automatic.py\", line 261, in _optimizer_step\n",
            "    call._call_lightning_module_hook(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/call.py\", line 142, in _call_lightning_module_hook\n",
            "    output = fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/core/module.py\", line 1265, in optimizer_step\n",
            "    optimizer.step(closure=optimizer_closure)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/core/optimizer.py\", line 158, in step\n",
            "    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/strategies/strategy.py\", line 224, in optimizer_step\n",
            "    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/plugins/precision/precision_plugin.py\", line 114, in optimizer_step\n",
            "    return optimizer.step(closure=closure, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/optim/lr_scheduler.py\", line 69, in wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/optim/optimizer.py\", line 280, in wrapper\n",
            "    out = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/deep_training/nlp/optimizer/lion/lion.py\", line 72, in step\n",
            "    loss = closure()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/plugins/precision/precision_plugin.py\", line 101, in _wrap_closure\n",
            "    closure_result = closure()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/optimization/automatic.py\", line 140, in __call__\n",
            "    self._result = self.closure(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/optimization/automatic.py\", line 126, in closure\n",
            "    step_output = self._step_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/optimization/automatic.py\", line 308, in _training_step\n",
            "    training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/call.py\", line 288, in _call_strategy_hook\n",
            "    output = fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/strategies/strategy.py\", line 366, in training_step\n",
            "    return self.model.training_step(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/deep_training/nlp/models/transformer_base.py\", line 529, in training_step\n",
            "    outputs = self.compute_loss(**batch)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/deep_training/nlp/models/transformer_base.py\", line 360, in compute_loss\n",
            "    return self.model.compute_loss(**kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/deep_training/nlp/models/transformer_base.py\", line 119, in compute_loss\n",
            "    return self.model(*args,**batch)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/deep_training/nlp/models/chatglm/__init__.py\", line 1204, in forward\n",
            "    transformer_outputs = self.transformer(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/deep_training/nlp/models/chatglm/__init__.py\", line 1019, in forward\n",
            "    layer_ret = layer(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/deep_training/nlp/models/chatglm/__init__.py\", line 629, in forward\n",
            "    attention_outputs = self.attention(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/deep_training/nlp/models/chatglm/__init__.py\", line 483, in forward\n",
            "    context_layer, present, attention_probs = attention_fn(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/deep_training/nlp/models/chatglm/__init__.py\", line 331, in attention_fn\n",
            "    attention_probs = attention_probs.type(dtype)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 0; 39.56 GiB total capacity; 38.38 GiB already allocated; 18.56 MiB free; 39.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch 0:  41%|████      | 120/295 [00:39<00:57,  3.05it/s, v_num=1, loss=1.960]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-PivCvSodauV"
      }
    }
  ]
}